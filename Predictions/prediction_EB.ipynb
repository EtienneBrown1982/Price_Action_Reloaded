{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from alpaca.data.requests import StockBarsRequest\n",
    "from alpaca.data.historical.stock import StockHistoricalDataClient\n",
    "from alpaca.data.timeframe import TimeFrame, TimeFrameUnit\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load .env file\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Alpaca API key and secret\n",
    "\n",
    "ALPACA_API_KEY = os.getenv(\"ALPACA_API_KEY\")\n",
    "ALPACA_SECRET_KEY = os.getenv(\"ALPACA_API_SECRET\")\n",
    "client = StockHistoricalDataClient(ALPACA_API_KEY, ALPACA_SECRET_KEY)\n",
    "\n",
    "# Create the Alpaca API object\n",
    "\n",
    "timeframe = TimeFrame(1, TimeFrameUnit.Day)\n",
    "symbol = 'SPY'\n",
    "start = datetime.utcnow() - timedelta(days=1600)\n",
    "end=datetime.utcnow() - timedelta(days=5)\n",
    "request = StockBarsRequest(symbol_or_symbols=symbol, start=start, end=end, timeframe=timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_df = client.get_stock_bars(request).df.tz_convert('America/New_York', level=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(source_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_df = source_df.copy()\n",
    "# pct_change is profit from last close\n",
    "bars_df[\"pct_change\"] = bars_df[\"close\"].pct_change()\n",
    "# signal for when we want to be in or out of a stock\n",
    "#bars_df[\"signal\"] = np.where(bars_df[\"pct_change\"] > 0, 1.0, 0.0)\n",
    "# reaction is the signal diff\n",
    "#bars_df[\"reaction\"] = bars_df[\"signal\"].diff()\n",
    "# action is if we could perfectly predict the next close\n",
    "#bars_df[\"action\"] = bars_df[\"reaction\"].shift(-1)\n",
    "# these values are the high, low, and open as a percentage of the current close\n",
    "bars_df[\"high %\"] = (bars_df[\"high\"] - bars_df[\"close\"])/bars_df[\"close\"]\n",
    "bars_df[\"low %\"] = (bars_df[\"low\"] - bars_df[\"close\"])/bars_df[\"close\"]\n",
    "bars_df[\"open %\"] = (bars_df[\"open\"] - bars_df[\"close\"])/bars_df[\"close\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bars_df.info()\n",
    "display(bars_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup DF for model\n",
    "\n",
    "bars_df = bars_df.droplevel(level=0).dropna()\n",
    "bars_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data set\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bars_df_scaled = scaler.fit_transform(bars_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "\n",
    "# input shape\n",
    "\n",
    "input_shape = bars_df_scaled.shape[1]\n",
    "latent_dim = 11\n",
    "num_samples = bars_df_scaled.shape[0]\n",
    "batch_size = 32\n",
    "\n",
    "# Generate Random Walk noise\n",
    "\n",
    "gaussian_noise = np.random.normal(0,1,size=(batch_size,input_shape))\n",
    "random_walk_noise = np.cumsum(gaussian_noise, axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define generator\n",
    "\n",
    "build_generator = Sequential([\n",
    "    Dense(128,input_shape=(input_shape,), activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(512, activation=\"relu\"),\n",
    "    Dense(input_shape, activation=\"sigmoid\")\n",
    "])\n",
    "\n",
    "# Define descriminator\n",
    "\n",
    "build_discriminator = Sequential([\n",
    "    Dense(512, input_shape=(input_shape,), activation=\"relu\"),\n",
    "    Dense(256, activation=\"relu\"),\n",
    "    Dense(128, activation=\"relu\"),\n",
    "    Dense(1, activation=\"sigmoid\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile generator\n",
    "build_generator.compile(loss = \"mse\", optimizer=\"adam\")\n",
    "\n",
    "# Compile discriminator\n",
    "build_discriminator.compile(loss=\"mse\", optimizer=\"adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine models\n",
    "\n",
    "z = tf.keras.Input(shape=(latent_dim,))\n",
    "img = build_generator(z)\n",
    "validity = build_discriminator(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define combined models\n",
    "\n",
    "combined = tf.keras.Model(z, validity)\n",
    "combined.compile(optimizer=\"adam\", loss=\"mse\", metrics=[\"accuracy\"])\n",
    "\n",
    "# Print summary of the combined model\n",
    "combined.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop\n",
    "epochs = 1000\n",
    "batch_size = 32\n",
    "num_samples = bars_df_scaled.shape[0]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Train discriminator\n",
    "    # Sample real data\n",
    "    idx = np.random.choice(num_samples, batch_size, replace=False)\n",
    "    real_data = bars_df_scaled[idx]\n",
    "    # Generate fake data\n",
    "    noise = random_walk_noise\n",
    "    fake_data = build_generator.predict(random_walk_noise)\n",
    "    # Train discriminator\n",
    "    d_loss_real = build_discriminator.train_on_batch(real_data, np.ones((batch_size, 1)))\n",
    "    d_loss_fake = build_discriminator.train_on_batch(fake_data, np.zeros((batch_size, 1)))\n",
    "    d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "    \n",
    "    # Train generator \n",
    "    noise = np.random.normal(0, 1, (batch_size, input_shape))\n",
    "    g_loss = combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "    \n",
    "    # Print progress\n",
    "    print(f\"Epoch {epoch}, Discriminator Loss: {d_loss}, Generator Loss: {g_loss}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create alpaca object for testing\n",
    "\n",
    "timeframe = TimeFrame(1, TimeFrameUnit.Day)\n",
    "symbol = 'SPY'\n",
    "start = datetime.utcnow() - timedelta(days=10)\n",
    "end=datetime.utcnow() - timedelta(days=3)\n",
    "request = StockBarsRequest(symbol_or_symbols=symbol, start=start, end=end, timeframe=timeframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create df for testing\n",
    "\n",
    "test_df = client.get_stock_bars(request).df.tz_convert('America/New_York', level=1)\n",
    "\n",
    "# data preprocessing for testing\n",
    "\n",
    "# pct_change is profit from last close\n",
    "test_df[\"pct_change\"] = test_df[\"close\"].pct_change()\n",
    "# signal for when we want to be in or out of a stock\n",
    "#bars_df[\"signal\"] = np.where(bars_df[\"pct_change\"] > 0, 1.0, 0.0)\n",
    "# reaction is the signal diff\n",
    "#bars_df[\"reaction\"] = bars_df[\"signal\"].diff()\n",
    "# action is if we could perfectly predict the next close\n",
    "#bars_df[\"action\"] = bars_df[\"reaction\"].shift(-1)\n",
    "# these values are the high, low, and open as a percentage of the current close\n",
    "test_df[\"high %\"] = (test_df[\"high\"] - test_df[\"close\"])/test_df[\"close\"]\n",
    "test_df[\"low %\"] = (test_df[\"low\"] - test_df[\"close\"])/test_df[\"close\"]\n",
    "test_df[\"open %\"] = (test_df[\"open\"] - test_df[\"close\"])/test_df[\"close\"]\n",
    "\n",
    "# set timestamp as index, drop nan\n",
    "\n",
    "test_df = test_df.droplevel(level=0).dropna()\n",
    "\n",
    "test_df.info()\n",
    "display(test_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize test data set\n",
    "\n",
    "test_df_scaled = scaler.fit_transform(test_df)\n",
    "\n",
    "print(test_df_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate discriminator on test data\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "discriminator_predictions = build_discriminator.predict(test_df_scaled)\n",
    "\n",
    "print(confusion_matrix(np.ones(len(test_df_scaled)),discriminator_predictions))\n",
    "print(classification_report(np.ones(len(test_df_scaled)),discriminator_predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "generated_data = build_generator.predict(random_walk_noise)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(generated_data[:4], label=\"generated data\")\n",
    "plt.xlabel(\"time\")\n",
    "plt.ylabel(\"value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original df shape\", test_df_scaled.shape)\n",
    "print(\"generated df shape\", generated_data.shape)\n",
    "\n",
    "print(\"Original dataframe summary statistics:\")\n",
    "print((test_df_scaled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generated dataframe summary statistics:\")\n",
    "print(pd.DataFrame(generated_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_data_inverse = scaler.inverse_transform(generated_data)\n",
    "generated_data_df = pd.DataFrame(generated_data_inverse)\n",
    "generated_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create alpaca object for testing\n",
    "\n",
    "timeframe = TimeFrame(1, TimeFrameUnit.Day)\n",
    "symbol = 'SPY'\n",
    "start = datetime.utcnow() - timedelta(days=3)\n",
    "end=datetime.utcnow() - timedelta(days=1)\n",
    "request = StockBarsRequest(symbol_or_symbols=symbol, start=start, end=end, timeframe=timeframe)\n",
    "\n",
    "current_df = client.get_stock_bars(request).df.tz_convert('America/New_York', level=1)\n",
    "current_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define constants\n",
    "\n",
    "# latent_dim = 100\n",
    "# input_shape = bars_df.shape[1]\n",
    "# output_shape = bars_df.shape[1]\n",
    "# num_samples = 1000\n",
    "# gaussian_noise = np.random.normal(0,1,size=(num_samples, latent_dim))\n",
    "# random_walk_noise = np.cumsum(gaussian_noise,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # define model building functions\n",
    "\n",
    "# def build_generator(num_layers,layer_size,activation,output_activation):\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.Dense(layer_size, activation = activation, input_dim=num_layers))\n",
    "\n",
    "#     for _ in range(num_layers-1):\n",
    "#         model.add(layers.Dense(layer_size, activation = activation))\n",
    "\n",
    "#     model.add(layers.Dense(output_shape,activation = output_activation))\n",
    "\n",
    "#     pass\n",
    "\n",
    "# # def build_discriminator(layers,layer_size,activation):\n",
    "\n",
    "# def build_discriminator(num_layers,layer_size,activation):\n",
    "#     model = models.Sequential()\n",
    "#     model.add(layers.Dense(layer_size, activation = activation, input_dim = input_shape))\n",
    "\n",
    "#     for _ in range(num_layers - 1):\n",
    "#         model.add(layers.Dense(layer_size, activation = activation))\n",
    "\n",
    "#     model.add(layers.Dense(1))\n",
    "\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data set\n",
    "\n",
    "scaler = StandardScaler()\n",
    "bars_df_scaled = scaler.fit_transform(bars_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "class GeneratorWrapper(BaseEstimator):\n",
    "    def __init__(self, num_layers, layer_size, activation, output_activation):\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_size = layer_size\n",
    "        self.activation = activation\n",
    "        self.output_activation = output_activation\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Build generator model\n",
    "        self.model = build_generator(self.num_layers, self.layer_size, self.activation, self.output_activation)\n",
    "        # Compile the model\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        # Fit the model\n",
    "        self.model.fit(X, X, epochs=3, batch_size=64, verbose=0)  # Assuming autoencoder-like training\n",
    "        return self\n",
    "\n",
    "class DiscriminatorWrapper(BaseEstimator):\n",
    "    def __init__(self, num_layers=2, layer_size=128, activation='relu'):\n",
    "        self.num_layers = num_layers\n",
    "        self.layer_size = layer_size\n",
    "        self.activation = activation\n",
    "        self.model = None\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # Build discriminator model\n",
    "        self.model = build_discriminator(self.num_layers, self.layer_size, self.activation)\n",
    "        # Compile the model\n",
    "        self.model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "        # Fit the model\n",
    "        self.model.fit(X, y, epochs=3, batch_size=64, verbose=0)  # Assuming binary classification\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Perform girdsearch for generator\n",
    "\n",
    "# generator_search = GridSearchCV(estimator=build_generator, param_grid=generator_grid, cv = 3)\n",
    "# generator_search.fit(random_walk_noise, bars_df_scaled)\n",
    "# best_generator_params = generator_search.best_params_\n",
    "\n",
    "# # get the best output layer activation from gridsearch\n",
    "\n",
    "# output_activation = best_generator_params.pop(\"output_activation\")\n",
    "\n",
    "# # Build the generator with the best parameters\n",
    "# generator = build_generator(latent_dim, output_shape, output_activation, **best_generator_params)\n",
    "\n",
    "# # Perform grid search for discriminator\n",
    "# discriminator_search = GridSearchCV(estimator=build_discriminator, param_grid=discriminator_grid, cv=3)\n",
    "# discriminator_search.fit(random_walk_noise,bars_df_scaled)\n",
    "# best_discriminator_params = discriminator_search.best_params_\n",
    "\n",
    "# # Build the discriminator with the best parameters\n",
    "# discriminator = build_discriminator(**best_discriminator_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create GridSearchCV instance for the generator\n",
    "generator_search = GridSearchCV(estimator=GeneratorWrapper(), param_grid=generator_grid, cv=3)\n",
    "generator_search.fit(random_walk_noise, bars_df_scaled)\n",
    "best_generator_params = generator_search.best_params_\n",
    "\n",
    "# Create GridSearchCV instance for the discriminator\n",
    "discriminator_search = GridSearchCV(estimator=DiscriminatorWrapper(), param_grid=discriminator_grid, cv=3)\n",
    "discriminator_search.fit(bars_df_scaled)\n",
    "best_discriminator_params = discriminator_search.best_params_\n",
    "\n",
    "print(\"Best parameters for the generator:\", best_generator_params)\n",
    "print(\"Best parameters for the discriminator:\", best_discriminator_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
